# Neural Networks From Scratch

The goal of this repository is to **recreate as many types of neural networks as possible** from scratch, without relying on high-level deep learning frameworks.

## ğŸ“Œ Current Progress
- âœ… Implemented a **basic Multilayer Perceptron (MLP)** capable of handling **binary classification** tasks.
- âœ… Implemented **Binary Cross Entropy** as well as **Cross Entropy** for multi class classification
- âœ… Implemented **ReLU** and **Sigmoid** activation functions
- âœ… Implemented **Mini-batches** for much faster training
- âœ… Implemented **Gradient descent with momentum**, **RMSProp** and **Adam** optimizers

## ğŸ› ï¸ To Do
- [X] Implement different **cost functions**  
- [X] Add various **optimizers** (Adam, RMSprop, etc.)  
- [X] Support multiple **activation functions**
- [ ] Implement **gradient clipping**
- [X] Implement **mini-batches**
- [ ] Implement different types of **regularization** (L1, L2, Dropout)
---
> Highest accuracy score achieved on the dev-set so far = 0.9823
---
> ğŸš€ This is an ongoing learning project aimed at understanding the inner workings of neural networks from the ground up.
