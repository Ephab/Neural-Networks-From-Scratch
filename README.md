# Neural Networks From Scratch

The goal of this repository is to **recreate as many types of neural networks as possible** from scratch, without relying on high-level deep learning frameworks.

## ğŸ“Œ Current Progress
âœ… Implemented a **basic Multilayer Perceptron (MLP)** capable of handling **binary classification** tasks.
âœ… Implemented Binary Cross Entropy as well as Cross Entropy for multi class classification
âœ… Implemented ReLU and Sigmoid activation functions

## ğŸ› ï¸ To Do
- [X] Implement different **cost functions**  
- [ ] Add various **optimizers** (Adam, RMSprop, etc.)  
- [X] Support multiple **activation functions**
- [ ] Implement **gradient clipping**
- [ ] Implement **mini batches**
- [ ] Implement different types of **regularization** (L1, L2, Dropout)
---

> ğŸš€ This is an ongoing learning project aimed at understanding the inner workings of neural networks from the ground up.
