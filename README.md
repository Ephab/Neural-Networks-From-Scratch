# Neural Networks From Scratch
![221352987-68da234d-4d62-4e9d-9d7f-098dc657c2dc](https://github.com/user-attachments/assets/765443ae-d538-4eec-9351-4d89a14ae7cd)


The goal of this repository is to **recreate as many types of neural networks as possible** from scratch, without relying on high-level deep learning frameworks.

## 📌 Current Progress
- ✅ Implemented a **basic Multilayer Perceptron (MLP)** capable of handling **binary classification** tasks.
- ✅ Implemented **Binary Cross Entropy** as well as **Cross Entropy** for multi class classification
- ✅ Implemented **ReLU** and **Sigmoid** activation functions
- ✅ Implemented **Mini-batches** for much faster training
- ✅ Implemented **Gradient descent with momentum**, **RMSProp** and **Adam** optimizers
- ✅ Implemented **He** and **Xavier** initializations
- ✅ Implemented **L2** and **Dropout** regularizations


## 🛠️ To Do
- [X] Implement different **cost functions**  
- [X] Add various **optimizers** (Adam, RMSprop, etc.)  
- [X] Support multiple **activation functions**
- [ ] Implement **gradient clipping**
- [X] Implement **mini-batches**
- [X] Implement different types of **regularization** (L1, L2, Dropout)
---
> Highest accuracy score achieved on the dev-set so far = 0.9823
---
> 🚀 This is an ongoing learning project aimed at understanding the inner workings of neural networks from the ground up.
