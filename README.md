# Neural Networks From Scratch
![221352987-68da234d-4d62-4e9d-9d7f-098dc657c2dc](https://github.com/user-attachments/assets/765443ae-d538-4eec-9351-4d89a14ae7cd)


The goal of this repository is to **recreate as many types of neural networks as possible** from scratch, without relying on high-level deep learning frameworks.

## ğŸ“Œ Current Progress
- âœ… Implemented a **basic Multilayer Perceptron (MLP)** capable of handling **binary classification** tasks.
- âœ… Implemented **Binary Cross Entropy** as well as **Cross Entropy** for multi class classification
- âœ… Implemented **ReLU** and **Sigmoid** activation functions
- âœ… Implemented **Mini-batches** for much faster training
- âœ… Implemented **Gradient descent with momentum**, **RMSProp** and **Adam** optimizers
- âœ… Implemented **He** and **Xavier** initializations
- âœ… Implemented **L2** and **Dropout** regularizations


## ğŸ› ï¸ To Do
- [X] Implement different **cost functions**  
- [X] Add various **optimizers** (Adam, RMSprop, etc.)  
- [X] Support multiple **activation functions**
- [ ] Implement **gradient clipping**
- [X] Implement **mini-batches**
- [X] Implement different types of **regularization** (L1, L2, Dropout)
---
> Highest accuracy score achieved on the dev-set so far = 0.9823
---
> ğŸš€ This is an ongoing learning project aimed at understanding the inner workings of neural networks from the ground up.
