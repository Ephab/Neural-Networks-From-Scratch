# Neural Networks From Scratch

The goal of this repository is to **recreate as many types of neural networks as possible** from scratch, without relying on high-level deep learning frameworks.

## 📌 Current Progress
✅ Implemented a **basic Multilayer Perceptron (MLP)** capable of handling **binary classification** tasks.
✅ Implemented Binary Cross Entropy as well as Cross Entropy for multi class classification
✅ Implemented ReLU and Sigmoid activation functions

## 🛠️ To Do
- [X] Implement different **cost functions**  
- [ ] Add various **optimizers** (Adam, RMSprop, etc.)  
- [X] Support multiple **activation functions**
- [ ] Implement **gradient clipping**
- [ ] Implement **mini batches**
- [ ] Implement different types of **regularization** (L1, L2, Dropout)
---

> 🚀 This is an ongoing learning project aimed at understanding the inner workings of neural networks from the ground up.
