{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {
        "id": "pGGkE4BAep1f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(suppress=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Binary classification\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # random dataset\n",
        "\n",
        "# NUM_SAMPLES_PER_CLASS = 200\n",
        "# INNER_RADIUS = 2\n",
        "# OUTER_RADIUS = 5\n",
        "# NOISE_STD_DEV = 0.5\n",
        "\n",
        "# theta_inner = 2 * np.pi * np.random.rand(NUM_SAMPLES_PER_CLASS)\n",
        "\n",
        "# radius_inner = INNER_RADIUS * np.random.rand(NUM_SAMPLES_PER_CLASS) + np.random.normal(0, NOISE_STD_DEV, NUM_SAMPLES_PER_CLASS)\n",
        "# x0 = radius_inner * np.cos(theta_inner)\n",
        "# y0 = radius_inner * np.sin(theta_inner)\n",
        "# class0_points = np.vstack([x0, y0]).T\n",
        "# class0_labels = np.zeros(NUM_SAMPLES_PER_CLASS)\n",
        "\n",
        "# theta_outer = 2 * np.pi * np.random.rand(NUM_SAMPLES_PER_CLASS)\n",
        "\n",
        "# radius_outer = np.random.uniform(INNER_RADIUS + 1, OUTER_RADIUS, NUM_SAMPLES_PER_CLASS) + np.random.normal(0, NOISE_STD_DEV, NUM_SAMPLES_PER_CLASS)\n",
        "# x1 = radius_outer * np.cos(theta_outer)\n",
        "# y1 = radius_outer * np.sin(theta_outer)\n",
        "# class1_points = np.vstack([x1, y1]).T\n",
        "# class1_labels = np.ones(NUM_SAMPLES_PER_CLASS)\n",
        "\n",
        "\n",
        "# X = np.vstack([class0_points, class1_points])\n",
        "# y = np.hstack([class0_labels, class1_labels])\n",
        "# y = y.reshape(-1, 1) # stupid broadcasting bug that i spent an hour on -.-\n",
        "\n",
        "\n",
        "# plt.figure(figsize=(8, 6))\n",
        "\n",
        "# plt.scatter(class0_points[:, 0], class0_points[:, 1], c='blue', label='Class 0 (Inner)', alpha=0.7)\n",
        "\n",
        "# plt.scatter(class1_points[:, 0], class1_points[:, 1], c='red', label='Class 1 (Outer)', alpha=0.7)\n",
        "# plt.title('Concentric Rings Dataset')\n",
        "# plt.xlabel('Feature 1')\n",
        "# plt.ylabel('Feature 2')\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.gca().set_aspect('equal', adjustable='box')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "0Wsu6mPtfNcy"
      },
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mnist dataset\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = (X_train.astype('float64') / 255.0).reshape(-1,28*28)\n",
        "X_test = (X_test.astype('float64') / 255.0).reshape(-1,28*28)\n",
        "\n",
        "X = X_train#[:2000,:]\n",
        "y = y_train.reshape(60000, -1)#[:2000,:]\n",
        "X_test = X_test#[:1000,:]\n",
        "y_test = y_test.reshape(-1,1)#[:1000, :]"
      ],
      "metadata": {
        "id": "_rV-2-fY5Ske"
      },
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y.shape, X_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z1MPju7fNj6",
        "outputId": "781365a2-de4b-4bed-edb0-618ee455b109"
      },
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 784), (60000, 1), (10000, 784), (10000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 321
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(vector):\n",
        "    vector = np.array(vector).astype(int).flatten()\n",
        "    # if vector.ndim == 1:\n",
        "    #     return vector\n",
        "    size=len(vector)\n",
        "    num_classes = max(np.max(vector) + 1, 10)\n",
        "\n",
        "    encoded_matrix = np.zeros((size, num_classes))\n",
        "    encoded_matrix[np.arange(size), vector] = 1\n",
        "    return encoded_matrix"
      ],
      "metadata": {
        "id": "wsCbSo0pwQ_l"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(X, y, test_size=0.2):\n",
        "    num_samples = X.shape[0]\n",
        "    train_size = int(np.ceil(num_samples * (1-test_size)))\n",
        "    test_size = int(np.floor(num_samples * test_size))\n",
        "\n",
        "    shuffled_indices = np.arange(num_samples)\n",
        "    np.random.shuffle(shuffled_indices)\n",
        "    X = X[shuffled_indices]\n",
        "    y= y[shuffled_indices]\n",
        "\n",
        "    X_train = X[:train_size, :]\n",
        "    y_train = y[:train_size,]\n",
        "    X_test = X[train_size:]\n",
        "    y_test = y[train_size:]\n",
        "\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "metadata": {
        "id": "fQo6YdzuK3sk"
      },
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, y_train, X_test, y_test = train_test_split(X, y)"
      ],
      "metadata": {
        "id": "DTUH6FEGOrcz"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights_and_biases(m_inputs, layer_sizes:list):\n",
        "    weights = {}\n",
        "    biases = {}\n",
        "\n",
        "    layers = [m_inputs] + layer_sizes\n",
        "\n",
        "    for i in range(len(layers)-1):\n",
        "        weights[\"W\" + str(i+1)] = np.random.randn(layers[i], layers[i+1])# * 0.01 left out due to vanishing gradients\n",
        "        biases[\"b\" + str(i+1)] = np.zeros((1, layers[i+1]))\n",
        "\n",
        "    # print(f\"Successfully initialized weights and biases for {len(layer_sizes)} layers\")\n",
        "    return weights, biases\n"
      ],
      "metadata": {
        "id": "pOIC9U5BfNo3"
      },
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0,z)\n",
        "\n",
        "def softmax(z):\n",
        "    exponent_values = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exponent_values / np.sum(exponent_values, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "9NQVMamKbrlu"
      },
      "execution_count": 326,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(A, weights, biases, activation_function):\n",
        "    Z_dict = {}\n",
        "    A_dict = {}\n",
        "    A_dict[\"A0\"] = A\n",
        "    l_dict = {}\n",
        "\n",
        "    if activation_function == 'relu':\n",
        "        activation_function = relu\n",
        "    else:\n",
        "        activation_function = sigmoid\n",
        "\n",
        "\n",
        "    for i in range(len(weights)-1): #loop through hidden layers\n",
        "        a_prev = A\n",
        "        Z = np.dot(a_prev, weights[\"W\" + str(i+1)]) + biases[\"b\" + str(i+1)]\n",
        "        Z_dict[\"Z\" + str(i+1)] = Z\n",
        "        A = activation_function(Z)\n",
        "        A_dict[\"A\" + str(i+1)] = A\n",
        "        l_dict['layer' + str(i+1)] = activation_function.__name__\n",
        "\n",
        "    last_index = len(weights)\n",
        "\n",
        "    if weights[list(weights.keys())[-1]].shape[1] == 1:\n",
        "        output_activation = sigmoid\n",
        "    else:\n",
        "        output_activation = softmax\n",
        "\n",
        "    a_prev = A\n",
        "    Z = np.dot(a_prev, weights[\"W\" + str(last_index)]) + biases[\"b\" + str(last_index)]\n",
        "    Z_dict[\"Z\" + str(last_index)] = Z\n",
        "    A = output_activation(Z)\n",
        "    A_dict[\"A\" + str(last_index)] = A\n",
        "    l_dict['layer' + str(last_index)] = output_activation.__name__\n",
        "\n",
        "\n",
        "    cache = {\"Z\":Z_dict,\n",
        "             \"A\":A_dict,\n",
        "             'layer_activations':l_dict}\n",
        "\n",
        "    # print(\"Successfully computed a forward pass\")\n",
        "    return cache"
      ],
      "metadata": {
        "id": "A_sdeVp1fNq3"
      },
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(y_pred, y_true, loss='binary_cross_entropy'):\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    if loss == 'cross_entropy':\n",
        "        return - np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))\n",
        "    else:\n",
        "        return - np.mean(y_true * np.log(y_pred + epsilon) + (1-y_true)*np.log(1-y_pred + epsilon)) # added 1e-8 to avoid log(0)"
      ],
      "metadata": {
        "id": "11iGNgXdoPaN"
      },
      "execution_count": 328,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_derivative(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)"
      ],
      "metadata": {
        "id": "Cpa39xURvtTr"
      },
      "execution_count": 329,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backpropagation(X, y, weights, biases, cache):\n",
        "\n",
        "    activation_derivative = sigmoid_derivative\n",
        "\n",
        "    A_cache = cache['A']\n",
        "    Z_cache = cache['Z']\n",
        "    L_cache = cache['layer_activations']\n",
        "\n",
        "    Z_gradients = {}\n",
        "    A_gradients = {}\n",
        "    W_gradients = {}\n",
        "    b_gradients = {}\n",
        "\n",
        "    m = X.shape[0]\n",
        "\n",
        "    for i in reversed(range(len(weights))):\n",
        "        if L_cache['layer' + str(i+1)] == 'relu':\n",
        "            activation_derivative = relu_derivative\n",
        "        if L_cache['layer' + str(i+1)] == 'sigmoid':\n",
        "            activation_derivative = sigmoid_derivative\n",
        "\n",
        "\n",
        "        if i+1 == len(weights): #special case for first dZ\n",
        "            dZ = A_cache[\"A\" + str(i+1)] - y\n",
        "            Z_gradients[\"dZ\" + str(i+1)] = dZ\n",
        "            W_gradients[\"dW\" + str(i+1)] = np.dot(A_cache[\"A\" + str(i)].T,dZ) / m\n",
        "            b_gradients[\"db\" + str(i+1)] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "        else:\n",
        "            dZ = np.dot(Z_gradients[\"dZ\" + str(i+2)], weights[\"W\" + str(i+2)].T ) * activation_derivative(Z_cache[\"Z\" + str(i+1)])\n",
        "            Z_gradients[\"dZ\" + str(i+1)] = dZ\n",
        "            W_gradients[\"dW\" + str(i+1)] = np.dot(A_cache[\"A\" + str(i)].T,dZ) / m\n",
        "            b_gradients[\"db\" + str(i+1)] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        # print(f\"Layer {i+1} dW magnitude: {np.linalg.norm(W_gradients['dW' + str(i+1)])}\")\n",
        "        # print(f\"Layer {i+1} db magnitude: {np.linalg.norm(b_gradients['db' + str(i+1)])}\")\n",
        "\n",
        "    grads = {\"dW\" : W_gradients,\n",
        "             \"db\" : b_gradients}\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "oBO-AJcSpSct"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_params(grads, weights, biases, learning_rate:float):\n",
        "\n",
        "    dw = grads['dW']\n",
        "    db = grads['db']\n",
        "\n",
        "    for i in range(len(weights)):\n",
        "        weights[\"W\" + str(i+1)] = weights[\"W\" + str(i+1)] - learning_rate * dw[\"dW\" + str(i+1)]\n",
        "        biases[\"b\" + str(i+1)] = biases[\"b\" + str(i+1)] - learning_rate * db[\"db\" + str(i+1)]\n",
        "\n",
        "    # print(\"Successfully updated params\")\n",
        "    return weights, biases"
      ],
      "metadata": {
        "id": "djF7hwUtxlyr"
      },
      "execution_count": 331,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, weights, biases):\n",
        "    cache = forward(X, weights, biases, activation_function='relu')\n",
        "    prediction = cache['A'][list(cache['A'].keys())[-1]]\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "uLU5V7b0y80T"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights, biases = initialize_weights_and_biases(m_inputs=784, layer_sizes=[16, 16, 10])"
      ],
      "metadata": {
        "id": "h6yc6ghG8j84"
      },
      "execution_count": 333,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if len(np.unique(y)) > 2:\n",
        "    y = one_hot_encoding(y)\n",
        "    y_test = one_hot_encoding(y_test)\n",
        "    classification_type = 'multi'\n",
        "else:\n",
        "    classification_type = 'single'\n",
        "\n",
        "\n",
        "def accuracy(prediction, labels):\n",
        "    total = len(labels)\n",
        "\n",
        "    if classification_type == 'single':\n",
        "        acc = np.sum((prediction > 0.5).astype(int) == labels) / total\n",
        "    else:\n",
        "        predicted_class = np.argmax(prediction, axis=1)\n",
        "        true_class = np.argmax(labels, axis=1)\n",
        "        acc = np.mean(predicted_class == true_class)\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "YAN6QKh0HlhD"
      },
      "execution_count": 334,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(epochs, learning_rate):\n",
        "    for i in range(epochs):\n",
        "        cache = forward(X, weights, biases, activation_function='relu')\n",
        "        prediction = cache['A'][list(cache['A'].keys())[-1]]\n",
        "        grads = backpropagation(X, y, weights, biases, cache)\n",
        "\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            dev_predictions = predict(X_test, weights, biases)\n",
        "            print(f\"epoch: {i}, loss= {calculate_loss(prediction, y, loss='cross_entropy' if classification_type=='multi' else 'binary_cross_entropy')}, training-set accuracy= {accuracy(prediction, y)}, dev-set accuracy= {accuracy(dev_predictions, y_test)}\")\n",
        "\n",
        "        update_params(grads, weights, biases, learning_rate)"
      ],
      "metadata": {
        "id": "bE-obYbZv2Lb"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_mini_batches(epochs, learning_rate, mini_batch_size=64):\n",
        "    num_samples = X.shape[0]\n",
        "    div = int(num_samples / mini_batch_size)\n",
        "\n",
        "    divisible_by_mini_batch_size = mini_batch_size * div\n",
        "    not_divisible = num_samples - divisible_by_mini_batch_size\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        shuffled_indices = np.arange(num_samples)\n",
        "        np.random.shuffle(shuffled_indices)\n",
        "        X_shuffled = X[shuffled_indices]\n",
        "        y_shuffled = y[shuffled_indices]\n",
        "\n",
        "        start = 0\n",
        "        end = 0\n",
        "        for batch in range(start, divisible_by_mini_batch_size, mini_batch_size):\n",
        "            start = end\n",
        "            end = start + mini_batch_size\n",
        "            X_batch = X_shuffled[start:end, :]\n",
        "            y_batch = y_shuffled[start:end]\n",
        "            #forward\n",
        "            cache = forward(X_batch, weights, biases, activation_function='relu')\n",
        "            prediction = cache['A'][list(cache['A'].keys())[-1]]\n",
        "            #backward\n",
        "            grads = backpropagation(X_batch, y_batch, weights, biases, cache)\n",
        "            #update\n",
        "            update_params(grads, weights, biases, learning_rate)\n",
        "\n",
        "        X_batch = X_shuffled[divisible_by_mini_batch_size:, :]\n",
        "        y_batch = y_shuffled[divisible_by_mini_batch_size:]\n",
        "        #forward\n",
        "        cache = forward(X_batch, weights, biases, activation_function='relu')\n",
        "        prediction = cache['A'][list(cache['A'].keys())[-1]]\n",
        "        #backward\n",
        "        grads = backpropagation(X_batch, y_batch, weights, biases, cache)\n",
        "        #update\n",
        "        update_params(grads, weights, biases, learning_rate)\n",
        "\n",
        "        cache = forward(X, weights, biases, activation_function='relu')\n",
        "        full_prediction = cache['A'][list(cache['A'].keys())[-1]]\n",
        "        dev_predictions = predict(X_test, weights, biases)\n",
        "        print(f\"epoch: {epoch}, loss= {calculate_loss(full_prediction, y, loss='cross_entropy' if classification_type=='multi' else 'binary_cross_entropy')}, training-set accuracy= {accuracy(full_prediction, y)}, dev-set accuracy= {accuracy(dev_predictions, y_test)}\")\n"
      ],
      "metadata": {
        "id": "AgzrO6SZBrIw"
      },
      "execution_count": 336,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run(epochs=10000, learning_rate=0.005)"
      ],
      "metadata": {
        "id": "4oMThHvuwDdK"
      },
      "execution_count": 337,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_mini_batches(epochs=100, learning_rate=0.05)"
      ],
      "metadata": {
        "id": "zq_460UVTJVT",
        "outputId": "46443b49-06a4-4cc4-e279-a876889752cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 338,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, loss= 1.7728505313944871, training-set accuracy= 0.28876666666666667, dev-set accuracy= 0.2868\n",
            "epoch: 1, loss= 1.6171721657596256, training-set accuracy= 0.34031666666666666, dev-set accuracy= 0.3401\n",
            "epoch: 2, loss= 1.5715715843585345, training-set accuracy= 0.36245, dev-set accuracy= 0.3666\n",
            "epoch: 3, loss= 1.420908521174914, training-set accuracy= 0.4352, dev-set accuracy= 0.4374\n",
            "epoch: 4, loss= 1.3408738339119226, training-set accuracy= 0.45553333333333335, dev-set accuracy= 0.4537\n",
            "epoch: 5, loss= 1.2773959732810278, training-set accuracy= 0.4924, dev-set accuracy= 0.4919\n",
            "epoch: 6, loss= 1.150438627568755, training-set accuracy= 0.5725333333333333, dev-set accuracy= 0.574\n",
            "epoch: 7, loss= 1.0163632658852015, training-set accuracy= 0.6493333333333333, dev-set accuracy= 0.6387\n",
            "epoch: 8, loss= 0.8896503724597555, training-set accuracy= 0.6888666666666666, dev-set accuracy= 0.6847\n",
            "epoch: 9, loss= 0.8045020786002308, training-set accuracy= 0.7325333333333334, dev-set accuracy= 0.7325\n",
            "epoch: 10, loss= 0.7377983018597462, training-set accuracy= 0.7567166666666667, dev-set accuracy= 0.7559\n",
            "epoch: 11, loss= 0.6455449728709084, training-set accuracy= 0.7869, dev-set accuracy= 0.7896\n",
            "epoch: 12, loss= 0.612637854174834, training-set accuracy= 0.7989166666666667, dev-set accuracy= 0.8003\n",
            "epoch: 13, loss= 0.5973632879683615, training-set accuracy= 0.8071, dev-set accuracy= 0.812\n",
            "epoch: 14, loss= 0.6830014579259136, training-set accuracy= 0.7779, dev-set accuracy= 0.7823\n",
            "epoch: 15, loss= 0.5740534059421296, training-set accuracy= 0.81155, dev-set accuracy= 0.8122\n",
            "epoch: 16, loss= 0.5641910751261681, training-set accuracy= 0.8198166666666666, dev-set accuracy= 0.8266\n",
            "epoch: 17, loss= 0.516904555613684, training-set accuracy= 0.8365333333333334, dev-set accuracy= 0.839\n",
            "epoch: 18, loss= 0.4790515853860989, training-set accuracy= 0.8519166666666667, dev-set accuracy= 0.8509\n",
            "epoch: 19, loss= 0.47321156088722277, training-set accuracy= 0.8493166666666667, dev-set accuracy= 0.8536\n",
            "epoch: 20, loss= 0.47800332095320264, training-set accuracy= 0.8481, dev-set accuracy= 0.8509\n",
            "epoch: 21, loss= 0.4388695746253652, training-set accuracy= 0.8642333333333333, dev-set accuracy= 0.8721\n",
            "epoch: 22, loss= 0.4265851451822307, training-set accuracy= 0.8677833333333334, dev-set accuracy= 0.8739\n",
            "epoch: 23, loss= 0.41223442059384613, training-set accuracy= 0.87585, dev-set accuracy= 0.8793\n",
            "epoch: 24, loss= 0.4211427477886664, training-set accuracy= 0.87005, dev-set accuracy= 0.8737\n",
            "epoch: 25, loss= 0.4448720355032732, training-set accuracy= 0.8610166666666667, dev-set accuracy= 0.8645\n",
            "epoch: 26, loss= 0.4097567625561731, training-set accuracy= 0.87465, dev-set accuracy= 0.8782\n",
            "epoch: 27, loss= 0.3780438845031869, training-set accuracy= 0.88695, dev-set accuracy= 0.8891\n",
            "epoch: 28, loss= 0.37812565840712753, training-set accuracy= 0.8852333333333333, dev-set accuracy= 0.8888\n",
            "epoch: 29, loss= 0.356592386001152, training-set accuracy= 0.8934166666666666, dev-set accuracy= 0.8991\n",
            "epoch: 30, loss= 0.3925779420204681, training-set accuracy= 0.8782, dev-set accuracy= 0.8847\n",
            "epoch: 31, loss= 0.366178961242628, training-set accuracy= 0.8888333333333334, dev-set accuracy= 0.8874\n",
            "epoch: 32, loss= 0.359962952815778, training-set accuracy= 0.88985, dev-set accuracy= 0.8929\n",
            "epoch: 33, loss= 0.354171943493326, training-set accuracy= 0.89245, dev-set accuracy= 0.8947\n",
            "epoch: 34, loss= 0.3683305161305088, training-set accuracy= 0.8881666666666667, dev-set accuracy= 0.8904\n",
            "epoch: 35, loss= 0.33638283511709804, training-set accuracy= 0.8997166666666667, dev-set accuracy= 0.9015\n",
            "epoch: 36, loss= 0.33658462914042436, training-set accuracy= 0.8988, dev-set accuracy= 0.9044\n",
            "epoch: 37, loss= 0.3647214781189523, training-set accuracy= 0.8882833333333333, dev-set accuracy= 0.8942\n",
            "epoch: 38, loss= 0.3309181028837817, training-set accuracy= 0.90225, dev-set accuracy= 0.9046\n",
            "epoch: 39, loss= 0.33466052277597363, training-set accuracy= 0.9010166666666667, dev-set accuracy= 0.9067\n",
            "epoch: 40, loss= 0.41094433281998516, training-set accuracy= 0.8706666666666667, dev-set accuracy= 0.87\n",
            "epoch: 41, loss= 0.3252666133977889, training-set accuracy= 0.9039666666666667, dev-set accuracy= 0.907\n",
            "epoch: 42, loss= 0.3470158589433086, training-set accuracy= 0.8946, dev-set accuracy= 0.8943\n",
            "epoch: 43, loss= 0.3260281204583411, training-set accuracy= 0.9021166666666667, dev-set accuracy= 0.9045\n",
            "epoch: 44, loss= 0.3034870555642402, training-set accuracy= 0.91045, dev-set accuracy= 0.911\n",
            "epoch: 45, loss= 0.3051462673450562, training-set accuracy= 0.9098666666666667, dev-set accuracy= 0.9136\n",
            "epoch: 46, loss= 0.30511073937058947, training-set accuracy= 0.91055, dev-set accuracy= 0.9143\n",
            "epoch: 47, loss= 0.3162435146243187, training-set accuracy= 0.90815, dev-set accuracy= 0.9125\n",
            "epoch: 48, loss= 0.31191447772826075, training-set accuracy= 0.9093, dev-set accuracy= 0.9125\n",
            "epoch: 49, loss= 0.34588713557314477, training-set accuracy= 0.8915166666666666, dev-set accuracy= 0.8908\n",
            "epoch: 50, loss= 0.3075051009180046, training-set accuracy= 0.9086166666666666, dev-set accuracy= 0.9103\n",
            "epoch: 51, loss= 0.28548623139941937, training-set accuracy= 0.9146833333333333, dev-set accuracy= 0.9179\n",
            "epoch: 52, loss= 0.3248227586123517, training-set accuracy= 0.9043166666666667, dev-set accuracy= 0.9061\n",
            "epoch: 53, loss= 0.30852331928379095, training-set accuracy= 0.9081, dev-set accuracy= 0.9084\n",
            "epoch: 54, loss= 0.30559830756852635, training-set accuracy= 0.9092, dev-set accuracy= 0.9114\n",
            "epoch: 55, loss= 0.3033160605525416, training-set accuracy= 0.9094166666666667, dev-set accuracy= 0.9108\n",
            "epoch: 56, loss= 0.2764100946415681, training-set accuracy= 0.9175833333333333, dev-set accuracy= 0.9192\n",
            "epoch: 57, loss= 0.3121654591061989, training-set accuracy= 0.9058333333333334, dev-set accuracy= 0.9061\n",
            "epoch: 58, loss= 0.30317716125627503, training-set accuracy= 0.9103166666666667, dev-set accuracy= 0.9103\n",
            "epoch: 59, loss= 0.3167076934106226, training-set accuracy= 0.9061666666666667, dev-set accuracy= 0.9076\n",
            "epoch: 60, loss= 0.29900252472761224, training-set accuracy= 0.9105833333333333, dev-set accuracy= 0.9127\n",
            "epoch: 61, loss= 0.2739966605165154, training-set accuracy= 0.9183, dev-set accuracy= 0.9196\n",
            "epoch: 62, loss= 0.3032966447241669, training-set accuracy= 0.9086833333333333, dev-set accuracy= 0.9066\n",
            "epoch: 63, loss= 0.2762604601715213, training-set accuracy= 0.918, dev-set accuracy= 0.919\n",
            "epoch: 64, loss= 0.2841090118613749, training-set accuracy= 0.9146, dev-set accuracy= 0.9162\n",
            "epoch: 65, loss= 0.2966845442367958, training-set accuracy= 0.9115666666666666, dev-set accuracy= 0.9106\n",
            "epoch: 66, loss= 0.28867282196932126, training-set accuracy= 0.91335, dev-set accuracy= 0.9156\n",
            "epoch: 67, loss= 0.27772832058395674, training-set accuracy= 0.9164, dev-set accuracy= 0.9165\n",
            "epoch: 68, loss= 0.2834472511334471, training-set accuracy= 0.9156166666666666, dev-set accuracy= 0.9153\n",
            "epoch: 69, loss= 0.279671518019944, training-set accuracy= 0.9162166666666667, dev-set accuracy= 0.9173\n",
            "epoch: 70, loss= 0.25700475095852143, training-set accuracy= 0.9235166666666667, dev-set accuracy= 0.9244\n",
            "epoch: 71, loss= 0.25927360991682835, training-set accuracy= 0.9235, dev-set accuracy= 0.9248\n",
            "epoch: 72, loss= 0.282949120902781, training-set accuracy= 0.9158, dev-set accuracy= 0.9144\n",
            "epoch: 73, loss= 0.26901356560391665, training-set accuracy= 0.9205166666666666, dev-set accuracy= 0.9211\n",
            "epoch: 74, loss= 0.26725089623421944, training-set accuracy= 0.9195166666666666, dev-set accuracy= 0.9218\n",
            "epoch: 75, loss= 0.26410426679350796, training-set accuracy= 0.9203666666666667, dev-set accuracy= 0.9206\n",
            "epoch: 76, loss= 0.28364508115246145, training-set accuracy= 0.9151166666666667, dev-set accuracy= 0.9123\n",
            "epoch: 77, loss= 0.2557853650124583, training-set accuracy= 0.9245833333333333, dev-set accuracy= 0.9255\n",
            "epoch: 78, loss= 0.28060059993101105, training-set accuracy= 0.9166, dev-set accuracy= 0.9131\n",
            "epoch: 79, loss= 0.29093148371819594, training-set accuracy= 0.9120833333333334, dev-set accuracy= 0.9113\n",
            "epoch: 80, loss= 0.2503761075038178, training-set accuracy= 0.9253666666666667, dev-set accuracy= 0.9242\n",
            "epoch: 81, loss= 0.28882607185501724, training-set accuracy= 0.91115, dev-set accuracy= 0.9082\n",
            "epoch: 82, loss= 0.2624534368827045, training-set accuracy= 0.9216333333333333, dev-set accuracy= 0.9228\n",
            "epoch: 83, loss= 0.26435943227171593, training-set accuracy= 0.9199833333333334, dev-set accuracy= 0.9199\n",
            "epoch: 84, loss= 0.24717484488281, training-set accuracy= 0.9279833333333334, dev-set accuracy= 0.9282\n",
            "epoch: 85, loss= 0.259431690904049, training-set accuracy= 0.9223, dev-set accuracy= 0.9214\n",
            "epoch: 86, loss= 0.26694960397697504, training-set accuracy= 0.9205, dev-set accuracy= 0.9194\n",
            "epoch: 87, loss= 0.24612080927245106, training-set accuracy= 0.9277833333333333, dev-set accuracy= 0.9264\n",
            "epoch: 88, loss= 0.2559794495192635, training-set accuracy= 0.9231333333333334, dev-set accuracy= 0.9245\n",
            "epoch: 89, loss= 0.2718767067909295, training-set accuracy= 0.9201666666666667, dev-set accuracy= 0.9196\n",
            "epoch: 90, loss= 0.2450170774865902, training-set accuracy= 0.9287833333333333, dev-set accuracy= 0.9263\n",
            "epoch: 91, loss= 0.25733698055165694, training-set accuracy= 0.9226833333333333, dev-set accuracy= 0.9195\n",
            "epoch: 92, loss= 0.2412837177075609, training-set accuracy= 0.9276833333333333, dev-set accuracy= 0.9256\n",
            "epoch: 93, loss= 0.2487642943721828, training-set accuracy= 0.9264333333333333, dev-set accuracy= 0.9245\n",
            "epoch: 94, loss= 0.25898986435508836, training-set accuracy= 0.9221333333333334, dev-set accuracy= 0.9201\n",
            "epoch: 95, loss= 0.25639504134033075, training-set accuracy= 0.9248333333333333, dev-set accuracy= 0.9229\n",
            "epoch: 96, loss= 0.23439280400657841, training-set accuracy= 0.9309833333333334, dev-set accuracy= 0.9291\n",
            "epoch: 97, loss= 0.2415950959490114, training-set accuracy= 0.9282666666666667, dev-set accuracy= 0.9281\n",
            "epoch: 98, loss= 0.2522415491989244, training-set accuracy= 0.924, dev-set accuracy= 0.9223\n",
            "epoch: 99, loss= 0.24534024486394335, training-set accuracy= 0.9261833333333334, dev-set accuracy= 0.923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez('trained_model_params.npz', **weights, **biases)\n",
        "# data = np.load('trained_model_params.npz')"
      ],
      "metadata": {
        "id": "PDZUH2uAUaeD"
      },
      "execution_count": 339,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize mnist\n",
        "\n",
        "def plot_predictions(X, y_true, y_pred, num_images=10):\n",
        "    \"\"\"\n",
        "    X is of shape (m, 28*28) where m is the # samples\n",
        "\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "U2S_jh2B9N5O"
      },
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# h = 0.05\n",
        "# x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
        "# y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
        "\n",
        "# xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "#                      np.arange(y_min, y_max, h))\n",
        "\n",
        "# grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "# preds = predict(grid_points, weights, biases)\n",
        "\n",
        "# Z = (preds > 0.5).astype(int)\n",
        "# Z = Z.reshape(xx.shape)\n",
        "\n",
        "\n",
        "# plt.figure(figsize=(8,6))\n",
        "# plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdBu)\n",
        "\n",
        "# plt.scatter(class0_points[:, 0], class0_points[:, 1], c='blue', label='Class 0 (Inner)', alpha=0.7)\n",
        "# plt.scatter(class1_points[:, 0], class1_points[:, 1], c='red', label='Class 1 (Outer)', alpha=0.7)\n",
        "\n",
        "# plt.title(\"Decision Boundary\")\n",
        "# plt.xlabel(\"Feature 1\")\n",
        "# plt.ylabel(\"Feature 2\")\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.gca().set_aspect('equal', adjustable='box')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "yoUkunx185nD"
      },
      "execution_count": 341,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2BUbd2vyiFKN"
      },
      "execution_count": 341,
      "outputs": []
    }
  ]
}