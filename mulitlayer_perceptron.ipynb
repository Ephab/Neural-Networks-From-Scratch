{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1558,
      "metadata": {
        "id": "pGGkE4BAep1f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(suppress=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Binary classification\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # random dataset\n",
        "\n",
        "# NUM_SAMPLES_PER_CLASS = 200\n",
        "# INNER_RADIUS = 2\n",
        "# OUTER_RADIUS = 5\n",
        "# NOISE_STD_DEV = 0.5\n",
        "\n",
        "# theta_inner = 2 * np.pi * np.random.rand(NUM_SAMPLES_PER_CLASS)\n",
        "\n",
        "# radius_inner = INNER_RADIUS * np.random.rand(NUM_SAMPLES_PER_CLASS) + np.random.normal(0, NOISE_STD_DEV, NUM_SAMPLES_PER_CLASS)\n",
        "# x0 = radius_inner * np.cos(theta_inner)\n",
        "# y0 = radius_inner * np.sin(theta_inner)\n",
        "# class0_points = np.vstack([x0, y0]).T\n",
        "# class0_labels = np.zeros(NUM_SAMPLES_PER_CLASS)\n",
        "\n",
        "# theta_outer = 2 * np.pi * np.random.rand(NUM_SAMPLES_PER_CLASS)\n",
        "\n",
        "# radius_outer = np.random.uniform(INNER_RADIUS + 1, OUTER_RADIUS, NUM_SAMPLES_PER_CLASS) + np.random.normal(0, NOISE_STD_DEV, NUM_SAMPLES_PER_CLASS)\n",
        "# x1 = radius_outer * np.cos(theta_outer)\n",
        "# y1 = radius_outer * np.sin(theta_outer)\n",
        "# class1_points = np.vstack([x1, y1]).T\n",
        "# class1_labels = np.ones(NUM_SAMPLES_PER_CLASS)\n",
        "\n",
        "\n",
        "# X = np.vstack([class0_points, class1_points])\n",
        "# y = np.hstack([class0_labels, class1_labels])\n",
        "# y = y.reshape(-1, 1) # stupid broadcasting bug that i spent an hour on -.-\n",
        "\n",
        "\n",
        "# plt.figure(figsize=(8, 6))\n",
        "\n",
        "# plt.scatter(class0_points[:, 0], class0_points[:, 1], c='blue', label='Class 0 (Inner)', alpha=0.7)\n",
        "\n",
        "# plt.scatter(class1_points[:, 0], class1_points[:, 1], c='red', label='Class 1 (Outer)', alpha=0.7)\n",
        "# plt.title('Concentric Rings Dataset')\n",
        "# plt.xlabel('Feature 1')\n",
        "# plt.ylabel('Feature 2')\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.gca().set_aspect('equal', adjustable='box')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "0Wsu6mPtfNcy"
      },
      "execution_count": 1559,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mnist dataset\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = (X_train.astype('float64') / 255.0).reshape(-1,28*28)\n",
        "X_test = (X_test.astype('float64') / 255.0).reshape(-1,28*28)\n",
        "\n",
        "X = X_train[:1000,:]\n",
        "y = y_train.reshape(60000, -1)[:1000,:]\n",
        "X_test = X_test[:1000,:]\n",
        "y_test = y_test.reshape(-1,1)[:1000, :]"
      ],
      "metadata": {
        "id": "_rV-2-fY5Ske"
      },
      "execution_count": 1560,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y.shape, X_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z1MPju7fNj6",
        "outputId": "b724d776-deb2-4162-b3c0-5b16f653fe8a"
      },
      "execution_count": 1561,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 784), (1000, 1), (1000, 784), (1000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 1561
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(vector):\n",
        "    vector = np.array(vector).astype(int).flatten()\n",
        "    # if vector.ndim == 1:\n",
        "    #     return vector\n",
        "    size=len(vector)\n",
        "    num_classes = max(np.max(vector) + 1, 10)\n",
        "\n",
        "    encoded_matrix = np.zeros((size, num_classes))\n",
        "    encoded_matrix[np.arange(size), vector] = 1\n",
        "    return encoded_matrix"
      ],
      "metadata": {
        "id": "wsCbSo0pwQ_l"
      },
      "execution_count": 1562,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(X, y, test_size=0.2):\n",
        "    num_samples = X.shape[0]\n",
        "    train_size = int(np.ceil(num_samples * (1-test_size)))\n",
        "    test_size = int(np.floor(num_samples * test_size))\n",
        "\n",
        "    shuffled_indices = np.arange(num_samples)\n",
        "    np.random.shuffle(shuffled_indices)\n",
        "    X = X[shuffled_indices]\n",
        "    y= y[shuffled_indices]\n",
        "\n",
        "    X_train = X[:train_size, :]\n",
        "    y_train = y[:train_size,]\n",
        "    X_test = X[train_size:]\n",
        "    y_test = y[train_size:]\n",
        "\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "metadata": {
        "id": "fQo6YdzuK3sk"
      },
      "execution_count": 1563,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, y_train, X_test, y_test = train_test_split(X, y)"
      ],
      "metadata": {
        "id": "DTUH6FEGOrcz"
      },
      "execution_count": 1564,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights_and_biases(m_inputs, layer_sizes:list):\n",
        "    weights = {}\n",
        "    biases = {}\n",
        "\n",
        "    layers = [m_inputs] + layer_sizes\n",
        "\n",
        "    for i in range(len(layers)-1):\n",
        "        weights[\"W\" + str(i+1)] = np.random.randn(layers[i], layers[i+1])# * 0.01 left out due to vanishing gradients\n",
        "        biases[\"b\" + str(i+1)] = np.zeros((1, layers[i+1]))\n",
        "\n",
        "    # print(f\"Successfully initialized weights and biases for {len(layer_sizes)} layers\")\n",
        "    return weights, biases\n"
      ],
      "metadata": {
        "id": "pOIC9U5BfNo3"
      },
      "execution_count": 1565,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0,z)\n",
        "\n",
        "def softmax(z):\n",
        "    exponent_values = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exponent_values / np.sum(exponent_values, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "9NQVMamKbrlu"
      },
      "execution_count": 1566,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(A, weights, biases, activation_function):\n",
        "    Z_dict = {}\n",
        "    A_dict = {}\n",
        "    A_dict[\"A0\"] = A\n",
        "    l_dict = {}\n",
        "\n",
        "    if activation_function == 'relu':\n",
        "        activation_function = relu\n",
        "    else:\n",
        "        activation_function = sigmoid\n",
        "\n",
        "\n",
        "    for i in range(len(weights)-1): #loop through hidden layers\n",
        "        a_prev = A\n",
        "        Z = np.dot(a_prev, weights[\"W\" + str(i+1)]) + biases[\"b\" + str(i+1)]\n",
        "        Z_dict[\"Z\" + str(i+1)] = Z\n",
        "        A = activation_function(Z)\n",
        "        A_dict[\"A\" + str(i+1)] = A\n",
        "        l_dict['layer' + str(i+1)] = activation_function.__name__\n",
        "\n",
        "    last_index = len(weights)\n",
        "\n",
        "    if weights[list(weights.keys())[-1]].shape[1] == 1:\n",
        "        output_activation = sigmoid\n",
        "    else:\n",
        "        output_activation = softmax\n",
        "\n",
        "    a_prev = A\n",
        "    Z = np.dot(a_prev, weights[\"W\" + str(last_index)]) + biases[\"b\" + str(last_index)]\n",
        "    Z_dict[\"Z\" + str(last_index)] = Z\n",
        "    A = output_activation(Z)\n",
        "    A_dict[\"A\" + str(last_index)] = A\n",
        "    l_dict['layer' + str(last_index)] = output_activation.__name__\n",
        "\n",
        "\n",
        "    cache = {\"Z\":Z_dict,\n",
        "             \"A\":A_dict,\n",
        "             'layer_activations':l_dict}\n",
        "\n",
        "    # print(\"Successfully computed a forward pass\")\n",
        "    return cache"
      ],
      "metadata": {
        "id": "A_sdeVp1fNq3"
      },
      "execution_count": 1567,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(y_pred, y_true, loss='binary_cross_entropy'):\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    if loss == 'cross_entropy':\n",
        "        return - np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))\n",
        "    else:\n",
        "        return - np.mean(y_true * np.log(y_pred + epsilon) + (1-y_true)*np.log(1-y_pred + epsilon)) # added 1e-8 to avoid log(0)"
      ],
      "metadata": {
        "id": "11iGNgXdoPaN"
      },
      "execution_count": 1568,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_derivative(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)"
      ],
      "metadata": {
        "id": "Cpa39xURvtTr"
      },
      "execution_count": 1569,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backpropagation(X, y, weights, biases, cache):\n",
        "\n",
        "    activation_derivative = sigmoid_derivative\n",
        "\n",
        "    A_cache = cache['A']\n",
        "    Z_cache = cache['Z']\n",
        "    L_cache = cache['layer_activations']\n",
        "\n",
        "    Z_gradients = {}\n",
        "    A_gradients = {}\n",
        "    W_gradients = {}\n",
        "    b_gradients = {}\n",
        "\n",
        "    m = X.shape[0]\n",
        "\n",
        "    for i in reversed(range(len(weights))):\n",
        "        if L_cache['layer' + str(i+1)] == 'relu':\n",
        "            activation_derivative = relu_derivative\n",
        "        if L_cache['layer' + str(i+1)] == 'sigmoid':\n",
        "            activation_derivative = sigmoid_derivative\n",
        "\n",
        "\n",
        "        if i+1 == len(weights): #special case for first dZ\n",
        "            dZ = A_cache[\"A\" + str(i+1)] - y\n",
        "            Z_gradients[\"dZ\" + str(i+1)] = dZ\n",
        "            W_gradients[\"dW\" + str(i+1)] = np.dot(A_cache[\"A\" + str(i)].T,dZ) / m\n",
        "            b_gradients[\"db\" + str(i+1)] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "        else:\n",
        "            dZ = np.dot(Z_gradients[\"dZ\" + str(i+2)], weights[\"W\" + str(i+2)].T ) * activation_derivative(Z_cache[\"Z\" + str(i+1)])\n",
        "            Z_gradients[\"dZ\" + str(i+1)] = dZ\n",
        "            W_gradients[\"dW\" + str(i+1)] = np.dot(A_cache[\"A\" + str(i)].T,dZ) / m\n",
        "            b_gradients[\"db\" + str(i+1)] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        # print(f\"Layer {i+1} dW magnitude: {np.linalg.norm(W_gradients['dW' + str(i+1)])}\")\n",
        "        # print(f\"Layer {i+1} db magnitude: {np.linalg.norm(b_gradients['db' + str(i+1)])}\")\n",
        "\n",
        "    grads = {\"dW\" : W_gradients,\n",
        "             \"db\" : b_gradients}\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "oBO-AJcSpSct"
      },
      "execution_count": 1570,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_params(grads, weights, biases, learning_rate:float):\n",
        "\n",
        "    dw = grads['dW']\n",
        "    db = grads['db']\n",
        "\n",
        "    for i in range(len(weights)):\n",
        "        weights[\"W\" + str(i+1)] = weights[\"W\" + str(i+1)] - learning_rate * dw[\"dW\" + str(i+1)]\n",
        "        biases[\"b\" + str(i+1)] = biases[\"b\" + str(i+1)] - learning_rate * db[\"db\" + str(i+1)]\n",
        "\n",
        "    # print(\"Successfully updated params\")\n",
        "    return weights, biases"
      ],
      "metadata": {
        "id": "djF7hwUtxlyr"
      },
      "execution_count": 1571,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, weights, biases):\n",
        "    cache = forward(X, weights, biases, activation_function='relu')\n",
        "    prediction = cache['A'][list(cache['A'].keys())[-1]]\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "uLU5V7b0y80T"
      },
      "execution_count": 1572,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights, biases = initialize_weights_and_biases(m_inputs=784, layer_sizes=[64, 32, 10])"
      ],
      "metadata": {
        "id": "h6yc6ghG8j84"
      },
      "execution_count": 1573,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if len(np.unique(y)) > 2:\n",
        "    y = one_hot_encoding(y)\n",
        "    y_test = one_hot_encoding(y_test)\n",
        "    classification_type = 'multi'\n",
        "else:\n",
        "    classification_type = 'single'\n",
        "\n",
        "\n",
        "def accuracy(prediction, labels):\n",
        "    total = len(labels)\n",
        "\n",
        "    if classification_type == 'single':\n",
        "        acc = np.sum((prediction > 0.5).astype(int) == labels) / total\n",
        "    else:\n",
        "        predicted_class = np.argmax(prediction, axis=1)\n",
        "        true_class = np.argmax(labels, axis=1)\n",
        "        acc = np.mean(predicted_class == true_class)\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "YAN6QKh0HlhD"
      },
      "execution_count": 1574,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(epochs, learning_rate):\n",
        "    for i in range(epochs):\n",
        "        cache = forward(X, weights, biases, activation_function='relu')\n",
        "        prediction = cache['A'][list(cache['A'].keys())[-1]]\n",
        "        grads = backpropagation(X, y, weights, biases, cache)\n",
        "\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            dev_predictions = predict(X_test, weights, biases)\n",
        "            print(f\"epoch: {i}, loss= {calculate_loss(prediction, y, loss='cross_entropy' if classification_type=='multi' else 'binary_cross_entropy')}, training-set accuracy= {accuracy(prediction, y)}, dev-set accuracy= {accuracy(dev_predictions, y_test)}\")\n",
        "\n",
        "        update_params(grads, weights, biases, learning_rate)"
      ],
      "metadata": {
        "id": "bE-obYbZv2Lb"
      },
      "execution_count": 1575,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run(epochs=10000, learning_rate=0.005)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oMThHvuwDdK",
        "outputId": "462783e9-278e-450a-8e5f-89e973c95a01"
      },
      "execution_count": 1576,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, loss= 16.3091260937568, training-set accuracy= 0.112, dev-set accuracy= 0.102\n",
            "epoch: 100, loss= 5.964900816110902, training-set accuracy= 0.603, dev-set accuracy= 0.452\n",
            "epoch: 200, loss= 3.767061793746855, training-set accuracy= 0.729, dev-set accuracy= 0.533\n",
            "epoch: 300, loss= 2.5349464122596785, training-set accuracy= 0.806, dev-set accuracy= 0.56\n",
            "epoch: 400, loss= 1.9226381256899896, training-set accuracy= 0.838, dev-set accuracy= 0.568\n",
            "epoch: 500, loss= 1.5338633967414275, training-set accuracy= 0.871, dev-set accuracy= 0.577\n",
            "epoch: 600, loss= 1.2636282869258713, training-set accuracy= 0.888, dev-set accuracy= 0.578\n",
            "epoch: 700, loss= 1.0271532616819328, training-set accuracy= 0.906, dev-set accuracy= 0.579\n",
            "epoch: 800, loss= 0.8084660665601533, training-set accuracy= 0.923, dev-set accuracy= 0.575\n",
            "epoch: 900, loss= 0.6308141084437351, training-set accuracy= 0.938, dev-set accuracy= 0.578\n",
            "epoch: 1000, loss= 0.48608853987311407, training-set accuracy= 0.951, dev-set accuracy= 0.58\n",
            "epoch: 1100, loss= 0.3940597149477061, training-set accuracy= 0.963, dev-set accuracy= 0.577\n",
            "epoch: 1200, loss= 0.3222120845387634, training-set accuracy= 0.969, dev-set accuracy= 0.578\n",
            "epoch: 1300, loss= 0.2630264215022537, training-set accuracy= 0.975, dev-set accuracy= 0.578\n",
            "epoch: 1400, loss= 0.2008810037904394, training-set accuracy= 0.981, dev-set accuracy= 0.585\n",
            "epoch: 1500, loss= 0.1646350953479897, training-set accuracy= 0.988, dev-set accuracy= 0.586\n",
            "epoch: 1600, loss= 0.13936352476689093, training-set accuracy= 0.99, dev-set accuracy= 0.584\n",
            "epoch: 1700, loss= 0.11854625648622028, training-set accuracy= 0.991, dev-set accuracy= 0.585\n",
            "epoch: 1800, loss= 0.10225967675405787, training-set accuracy= 0.994, dev-set accuracy= 0.588\n",
            "epoch: 1900, loss= 0.0904223969112954, training-set accuracy= 0.995, dev-set accuracy= 0.589\n",
            "epoch: 2000, loss= 0.08235197202359748, training-set accuracy= 0.995, dev-set accuracy= 0.591\n",
            "epoch: 2100, loss= 0.07774163984035949, training-set accuracy= 0.996, dev-set accuracy= 0.592\n",
            "epoch: 2200, loss= 0.07418584256482379, training-set accuracy= 0.996, dev-set accuracy= 0.593\n",
            "epoch: 2300, loss= 0.07106860048718601, training-set accuracy= 0.996, dev-set accuracy= 0.591\n",
            "epoch: 2400, loss= 0.06811042166188838, training-set accuracy= 0.996, dev-set accuracy= 0.592\n",
            "epoch: 2500, loss= 0.0640307689273042, training-set accuracy= 0.996, dev-set accuracy= 0.592\n",
            "epoch: 2600, loss= 0.05505108517078464, training-set accuracy= 0.996, dev-set accuracy= 0.593\n",
            "epoch: 2700, loss= 0.04516989045758281, training-set accuracy= 0.996, dev-set accuracy= 0.593\n",
            "epoch: 2800, loss= 0.03569440725518212, training-set accuracy= 0.996, dev-set accuracy= 0.593\n",
            "epoch: 2900, loss= 0.028813006924391586, training-set accuracy= 0.997, dev-set accuracy= 0.593\n",
            "epoch: 3000, loss= 0.023666445183403047, training-set accuracy= 0.997, dev-set accuracy= 0.592\n",
            "epoch: 3100, loss= 0.01924046018739744, training-set accuracy= 0.997, dev-set accuracy= 0.592\n",
            "epoch: 3200, loss= 0.015503287799362024, training-set accuracy= 0.998, dev-set accuracy= 0.592\n",
            "epoch: 3300, loss= 0.012543726114282571, training-set accuracy= 0.998, dev-set accuracy= 0.592\n",
            "epoch: 3400, loss= 0.010354412206684229, training-set accuracy= 0.999, dev-set accuracy= 0.593\n",
            "epoch: 3500, loss= 0.008788022541281497, training-set accuracy= 0.999, dev-set accuracy= 0.593\n",
            "epoch: 3600, loss= 0.007647195631236798, training-set accuracy= 1.0, dev-set accuracy= 0.593\n",
            "epoch: 3700, loss= 0.006787786890305368, training-set accuracy= 1.0, dev-set accuracy= 0.593\n",
            "epoch: 3800, loss= 0.006115862271433724, training-set accuracy= 1.0, dev-set accuracy= 0.593\n",
            "epoch: 3900, loss= 0.005574317700252313, training-set accuracy= 1.0, dev-set accuracy= 0.593\n",
            "epoch: 4000, loss= 0.005127792508999964, training-set accuracy= 1.0, dev-set accuracy= 0.594\n",
            "epoch: 4100, loss= 0.00475252756844337, training-set accuracy= 1.0, dev-set accuracy= 0.594\n",
            "epoch: 4200, loss= 0.004432133998276125, training-set accuracy= 1.0, dev-set accuracy= 0.594\n",
            "epoch: 4300, loss= 0.0041552518094522635, training-set accuracy= 1.0, dev-set accuracy= 0.594\n",
            "epoch: 4400, loss= 0.0039131066613866785, training-set accuracy= 1.0, dev-set accuracy= 0.594\n",
            "epoch: 4500, loss= 0.0036994816933134606, training-set accuracy= 1.0, dev-set accuracy= 0.594\n",
            "epoch: 4600, loss= 0.0035094687029300583, training-set accuracy= 1.0, dev-set accuracy= 0.594\n",
            "epoch: 4700, loss= 0.003339588882673851, training-set accuracy= 1.0, dev-set accuracy= 0.594\n",
            "epoch: 4800, loss= 0.0031864568134684895, training-set accuracy= 1.0, dev-set accuracy= 0.596\n",
            "epoch: 4900, loss= 0.0030477597458003973, training-set accuracy= 1.0, dev-set accuracy= 0.596\n",
            "epoch: 5000, loss= 0.0029213123433398866, training-set accuracy= 1.0, dev-set accuracy= 0.596\n",
            "epoch: 5100, loss= 0.0028055842379624972, training-set accuracy= 1.0, dev-set accuracy= 0.597\n",
            "epoch: 5200, loss= 0.0026992254971298373, training-set accuracy= 1.0, dev-set accuracy= 0.597\n",
            "epoch: 5300, loss= 0.002601050391437627, training-set accuracy= 1.0, dev-set accuracy= 0.597\n",
            "epoch: 5400, loss= 0.0025101386528879445, training-set accuracy= 1.0, dev-set accuracy= 0.598\n",
            "epoch: 5500, loss= 0.002425718761158594, training-set accuracy= 1.0, dev-set accuracy= 0.598\n",
            "epoch: 5600, loss= 0.0023470507154640043, training-set accuracy= 1.0, dev-set accuracy= 0.598\n",
            "epoch: 5700, loss= 0.0022736829601439383, training-set accuracy= 1.0, dev-set accuracy= 0.599\n",
            "epoch: 5800, loss= 0.0022049699572728814, training-set accuracy= 1.0, dev-set accuracy= 0.599\n",
            "epoch: 5900, loss= 0.0021405853994144164, training-set accuracy= 1.0, dev-set accuracy= 0.599\n",
            "epoch: 6000, loss= 0.0020802558783983597, training-set accuracy= 1.0, dev-set accuracy= 0.599\n",
            "epoch: 6100, loss= 0.002023335813891392, training-set accuracy= 1.0, dev-set accuracy= 0.599\n",
            "epoch: 6200, loss= 0.0019696268293999944, training-set accuracy= 1.0, dev-set accuracy= 0.599\n",
            "epoch: 6300, loss= 0.0019189232074746853, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 6400, loss= 0.0018708760776036287, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 6500, loss= 0.0018253846924779795, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 6600, loss= 0.0017821072742222974, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 6700, loss= 0.0017409668549222394, training-set accuracy= 1.0, dev-set accuracy= 0.599\n",
            "epoch: 6800, loss= 0.001701750474967085, training-set accuracy= 1.0, dev-set accuracy= 0.599\n",
            "epoch: 6900, loss= 0.0016643273260331182, training-set accuracy= 1.0, dev-set accuracy= 0.599\n",
            "epoch: 7000, loss= 0.0016285860236297243, training-set accuracy= 1.0, dev-set accuracy= 0.599\n",
            "epoch: 7100, loss= 0.0015943634497882588, training-set accuracy= 1.0, dev-set accuracy= 0.599\n",
            "epoch: 7200, loss= 0.001561597063331162, training-set accuracy= 1.0, dev-set accuracy= 0.599\n",
            "epoch: 7300, loss= 0.0015301902129787064, training-set accuracy= 1.0, dev-set accuracy= 0.599\n",
            "epoch: 7400, loss= 0.0015000687427872876, training-set accuracy= 1.0, dev-set accuracy= 0.599\n",
            "epoch: 7500, loss= 0.0014711597339749111, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 7600, loss= 0.001443361608211908, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 7700, loss= 0.001416635017278865, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 7800, loss= 0.0013908977902770921, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 7900, loss= 0.0013661055439714686, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 8000, loss= 0.0013422060434795486, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 8100, loss= 0.001319169009243606, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 8200, loss= 0.0012969420160960166, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 8300, loss= 0.0012754708077229683, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 8400, loss= 0.0012547226586358973, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 8500, loss= 0.001234659458387098, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 8600, loss= 0.001215247838581768, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 8700, loss= 0.0011964618378493535, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 8800, loss= 0.001178268685125058, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 8900, loss= 0.0011606308855995641, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 9000, loss= 0.001143529076272369, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 9100, loss= 0.0011269419956163684, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 9200, loss= 0.0011108386208762801, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 9300, loss= 0.0010952024343462222, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 9400, loss= 0.0010800171140612684, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 9500, loss= 0.0010652669938815062, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 9600, loss= 0.0010509267602494393, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 9700, loss= 0.0010369778182707332, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 9800, loss= 0.0010234158928607306, training-set accuracy= 1.0, dev-set accuracy= 0.6\n",
            "epoch: 9900, loss= 0.0010102004028260754, training-set accuracy= 1.0, dev-set accuracy= 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize mnist\n",
        "\n",
        "def plot_predictions(X, y_true, y_pred, num_images=10):\n",
        "    \"\"\"\n",
        "    X is of shape (m, 28*28) where m is the # samples\n",
        "\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "U2S_jh2B9N5O"
      },
      "execution_count": 1577,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# h = 0.05\n",
        "# x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
        "# y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
        "\n",
        "# xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "#                      np.arange(y_min, y_max, h))\n",
        "\n",
        "# grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "# preds = predict(grid_points, weights, biases)\n",
        "\n",
        "# Z = (preds > 0.5).astype(int)\n",
        "# Z = Z.reshape(xx.shape)\n",
        "\n",
        "\n",
        "# plt.figure(figsize=(8,6))\n",
        "# plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdBu)\n",
        "\n",
        "# plt.scatter(class0_points[:, 0], class0_points[:, 1], c='blue', label='Class 0 (Inner)', alpha=0.7)\n",
        "# plt.scatter(class1_points[:, 0], class1_points[:, 1], c='red', label='Class 1 (Outer)', alpha=0.7)\n",
        "\n",
        "# plt.title(\"Decision Boundary\")\n",
        "# plt.xlabel(\"Feature 1\")\n",
        "# plt.ylabel(\"Feature 2\")\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.gca().set_aspect('equal', adjustable='box')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "yoUkunx185nD"
      },
      "execution_count": 1578,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2BUbd2vyiFKN"
      },
      "execution_count": 1578,
      "outputs": []
    }
  ]
}